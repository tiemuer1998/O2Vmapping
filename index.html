<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>O2V-Mapping: Online Open-Vocabulary Mapping with Neural Implicit Representation</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon_o2v.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">O₂V-Mapping: Online Open-Vocabulary Mapping with Neural Implicit Representation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block"><a>Muer Tie</a><sup>1,2</sup>,</span>
              <span class="author-block"><a>Julong Wei</a><sup>1</sup>,</span>
              <span class="author-block"><a>Ke Wu</a><sup>1</sup>,</span>
              <span class="author-block"><a>Zhengjun Wang</a><sup>1</sup>,</span>
              <span class="author-block"><a>Shanshuai Yuan</a><sup>1</sup>,</span>
              <span class="author-block"><a>Kaizhao Zhang</a><sup>3</sup>,</span>
              <span class="author-block"><a>Jie Jia</a><sup>1</sup>,</span>
              <span class="author-block"><a>Jieru Zhao</a><sup>2</sup>,</span>
              <span class="author-block"><a>Zhongxue Gan</a><sup>1✉</sup>,</span>
              <span class="author-block"><a>Wenchao Ding</a><sup>1✉</sup></span>
            </div>

            <div class="is-size-5 publication-authors">
                <span class="author-block">
                  1 Academy for Engineering & Technology, Fudan University, China<br>
                  2 School of Future Technology, Harbin Institute of Technology, China<br>
                  3 Department of Computer Science and Engineering, Shanghai Jiao Tong University, China
                </span>
                <br>
                <span class="author-block">
                  <a href="mailto:metie22@m.fudan.edu.cn">metie22@m.fudan.edu.cn</a>
                </span>
                <br>
                <span class="eql-cntrb"><small><sup>✉</sup> Corresponding Authors</small></span>
            </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Fudan-MAGIC-Lab/O2Vmapping" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2404.06836" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/search.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Online O2V Mapping and Text-Based Search Results.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Online construction of open-ended language scenes is crucial for robotic applications, where open-vocabulary interactive scene understanding is required. Recently, neural implicit representation has provided a promising direction for online interactive mapping. However, implementing open-vocabulary scene understanding capability into online neural implicit mapping still faces three challenges: lack of local scene updating ability, blurry spatial hierarchical semantic segmentation and difficulty in maintaining multi-view consistency. To this end, we proposed O2V-mapping, which utilizes voxel-based language and geometric features to create an open-vocabulary field, thus allowing for local updates during online training process. Additionally, we leverage a foundational model for image segmentation to extract language features on object-level entities, achieving clear segmentation boundaries and hierarchical semantic features. For the purpose of preserving consistency in 3D object properties across different viewpoints, we propose a spatial adaptive voxel adjustment mechanism and a multi-view weight selection method. Extensive experiments on open-vocabulary object localization and semantic segmentation demonstrate that O2V-mapping achieves online construction of language scenes while enhancing accuracy, outperforming the previous SOTA method.We have now open-sourced our code at https://github.com/Fudan-MAGIC-Lab/O2Vmapping.git.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->




<!-- Static image display -->
<section class="hero is-small">
    <div class="hero-body">
      <div class="container has-text-centered">
        <!-- Your static image here -->
        <img src="static/images/pipline3.png" alt="MY ALT TEXT" style="max-width: 100%; height: auto;"/>
  
        <h2 class="subtitle has-text-centered" style="margin-top: 1em;">
          Top: Optimization of voxel-based neural
          radiance fields. Nearest trilinear interpolation is used to obtain color and geometric
          features for spatially sampled points. Then, leveraging NeRF’s volume rendering, the
          scene is sampled and average-rendered to produce RGB and depth images. <br><br>
          Bottom: Optimization of our O2V filed. We employ SAM to segment input RGB images and
          obtain instances. We further obtain language features through CLIP encoding. Feature
          indexing is performed to prepare for feature fusion. Finally, voxel splitting and multi-perspective voting are adopted to obtain fine-grained 3D open-vocabulary results.
        </h2>
      </div>
    </div>
  </section>
  



<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://youtu.be/zWirggX_hiA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->




<!-- Static image display -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>O2V Compare Slider x4</title>
  <style>
    .compare-block {
      max-width: 800px;
      margin: 40px auto;
      position: relative;
      height: 400px;
      overflow: hidden;
      cursor: ew-resize;
    }

    .compare-wrapper {
      position: absolute;
      top: 0;
      left: 0;
      height: 100%;
      width: 100%;
    }

    .compare-left, .compare-right {
      position: absolute;
      top: 0;
      height: 100%;
      overflow: hidden;
      width: 100%;
    }

    .compare-left img,
    .compare-right img {
      width: 100%;
      height: 100%;
      object-fit: cover;
      pointer-events: none;
    }

    .compare-left {
      z-index: 1;
    }

    .compare-right {
      z-index: 2;
    }

    .compare-divider {
      position: absolute;
      top: 0;
      width: 2px;
      height: 100%;
      background-color: white;
      z-index: 3;
    }

    .section-title {
      text-align: center;
      margin-top: 20px;
      font-size: 1.2em;
    }
  </style>
</head>
<body>

<div class="section-title">Comparison 1</div>
<div class="compare-block" id="compare1">
  <div class="compare-wrapper">
    <div class="compare-left"><img src="static/videos/left1-min-1.gif" alt="Left 1"></div>
    <div class="compare-right" id="right1"><img src="static/videos/right1.gif" alt="Right 1"></div>
    <div class="compare-divider" id="divider1"></div>
  </div>
</div>

<div class="section-title">Comparison 2</div>
<div class="compare-block" id="compare2">
  <div class="compare-wrapper">
    <div class="compare-left"><img src="static/videos/left2-min-1.gif" alt="Left 2"></div>
    <div class="compare-right" id="right2"><img src="static/videos/right2-min.gif" alt="Right 2"></div>
    <div class="compare-divider" id="divider2"></div>
  </div>
</div>

<div class="section-title">Comparison 3</div>
<div class="compare-block" id="compare3">
  <div class="compare-wrapper">
    <div class="compare-left"><img src="static/videos/left3-min-1.gif" alt="Left 3"></div>
    <div class="compare-right" id="right3"><img src="static/videos/right3-min.gif" alt="Right 3"></div>
    <div class="compare-divider" id="divider3"></div>
  </div>
</div>

<div class="section-title">Comparison 4</div>
<div class="compare-block" id="compare4">
  <div class="compare-wrapper">
    <div class="compare-left"><img src="static/videos/left4-min-1.gif" alt="Left 4"></div>
    <div class="compare-right" id="right4"><img src="static/videos/right4-min.gif" alt="Right 4"></div>
    <div class="compare-divider" id="divider4"></div>
  </div>
</div>

<script>
  function setupCompare(id) {
    const block = document.getElementById(id);
    const right = document.getElementById("right" + id.slice(-1));
    const divider = document.getElementById("divider" + id.slice(-1));

    block.addEventListener("mousemove", function (e) {
      const rect = block.getBoundingClientRect();
      const x = e.clientX - rect.left;
      const percent = Math.max(0, Math.min(x / rect.width, 1));
      right.style.clipPath = `inset(0 0 0 ${percent * 100}%)`;
      divider.style.left = `${percent * 100}%`;
    });
  }

  setupCompare("compare1");
  setupCompare("compare2");
  setupCompare("compare3");
  setupCompare("compare4");
</script>

</body>
</html>

<!-- End static image display -->





<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{tie20242,
        title={O2V-Mapping: Online Open-Vocabulary Mapping with Neural Implicit Representation},
        author={Tie, Muer and Wei, Julong and Wu, Ke and Wang, Zhengjun and Yuan, Shanshuai and Zhang, Kaizhao and Jia, Jie and Zhao, Jieru and Gan, Zhongxue and Ding, Wenchao},
        booktitle={European Conference on Computer Vision},
        pages={318--333},
        year={2024},
        organization={Springer}
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
